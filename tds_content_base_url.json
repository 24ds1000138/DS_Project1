[
  {
    "section": "../development-tools",
    "text": "Development Tools\nNOTE: The tools in this module are PRE-REQUISITES for the course. You would have used most of these before. If most of this is new to you, please take this course later.\nSome tools are fundamental to data science because they are industry standards and widely used by data science professionals. Mastering these tools will align you with current best practices and making you more adaptable in a fast-evolving industry.\nThe tools we cover here are not just popular, they’re the core technology behind most of today’s data science and software development."
  },
  {
    "section": "../vscode",
    "text": "Editor: VS Code\nYour editor is the most important tool in your arsenal. That’s where you’ll spend most of your time. Make sure you’re comfortable with it.\nVisual Studio Code is, by far, the most popular code editor today. According to the 2024 StackOverflow Survey almost 75% of developers use it. We recommend you learn it well. Even if you use another editor, you’ll be working with others who use it, and it’s a good idea to have some exposure.\nWatch these introductory videos (35 min) from the Visual Studio Docs to get started:\nGetting Started: Set up and learn the basics of Visual Studio Code. (7 min)\nCode Editing: Learn how to edit and run code in VS Code. (3 min)\nProductivity Tips: Become a VS Code power user with these productivity tips. (4 min)\nPersonalize: Personalize VS Code to make it yours with themes. (2 min)\nExtensions: Add features, themes, and more to VS Code with extensions! (4 min)\nDebugging: Get started with debugging in VS Code. (6 min)\nVersion Control: Learn how to use Git version control in VS Code. (3 min)\nCustomize: Learn how to customize your settings and keyboard shortcuts in VS Code. (6 min)"
  },
  {
    "section": "../uv",
    "text": "Python tools: uv\nInstall uv.\nuv is a fast Python package and project manager that’s becoming the standard for running Python scripts. It replaces tools like pip, conda, pipx, poetry, pyenv, twine, and virtualenv into one, enabling:\nPython Version Management: uv installs and manages multiple Python versions, allowing developers to specify and switch between versions seamlessly.\nVirtual Environment Handling: It automates the creation and management of virtual environments, ensuring isolated and consistent development spaces for different projects.\nDependency Management: With support for the pyproject.toml format, uv enables precise specification of project dependencies. It maintains a universal lockfile, uv.lock, to ensure reproducible installations across different systems.\nProject Execution: The uv run command allows for the execution of scripts and applications within the managed environment, streamlining development workflows.\nHere are some commonly used commands:\n# Replace python with uv. This automatically installs Python and dependencies.\nuv run script.py\n\n# Run a Python script directly from the Internet\nuv run https://example.com/script.py\n\n# Run a Python script without installing\nuvx ruff\n\n# Use a specific Python version\nuv run --python 3.11 script.py\n\n# Add dependencies to your script\nuv add httpx --script script.py\n\n# Create a virtual environment at .venv\nuv venv\n\n# Install packages to your virtual environment\nuv pip install httpx\nHere are some useful tools you can run with uvx without installation:\nuvx --from jupyterlab jupyter-lab   # Jupyter notebook\nuvx marimo      # Interactive notebook\nuvx llm         # Chat with LLMs from the command line\nuvx openwebui   # Chat with LLMs via the browser\nuvx httpie      # Make HTTP requests\nuvx datasette   # Browse SQLite databases\nuvx markitdown  # Convert PDF to Markdown\nuvx yt-dlp      # Download YouTube videos\nuvx asciinema   # Record your terminal and play it\nuv uses inline script metadata for dependencies. The eliminates the need for requirements.txt or virtual environments. For example:\n# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#   \"httpx\",\n#   \"pandas\",\n# ]\n# ///"
  },
  {
    "section": "../sqlite",
    "text": "Database: SQLite\nRelational databases are used to store data in a structured way. You’ll often access databases created by others for analysis.\nPostgreSQL, MySQL, MS SQL, Oracle, etc. are popular databases. But the most installed database is SQLite. It’s embedded into many devices and apps (e.g. your phone, browser, etc.). It’s lightweight but very scalable and powerful.\nWatch these introductory videos to understand SQLite and how it’s used in Python (34 min):\nThere are many non-relational databases (NoSQL) like ElasticSearch, MongoDB, Redis, etc. that you should know about and we may cover later.\nCore Concepts:\n-- Create a table\nCREATE TABLE users (\n    id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    email TEXT UNIQUE,\n    created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Insert data\nINSERT INTO users (name, email) VALUES\n    ('Alice', 'alice@example.com'),\n    ('Bob', 'bob@example.com');\n\n-- Query data\nSELECT name, COUNT(*) as count\nFROM users\nGROUP BY name\nHAVING count > 1;\n\n-- Join tables\nSELECT u.name, o.product\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE o.status = 'pending';\nPython Integration:\nimport sqlite3\nfrom pathlib import Path\nimport pandas as pd\n\nasync def query_database(db_path: Path, query: str) -> pd.DataFrame:\n    \"\"\"Execute SQL query and return results as DataFrame.\n\n    Args:\n        db_path: Path to SQLite database\n        query: SQL query to execute\n\n    Returns:\n        DataFrame with query results\n    \"\"\"\n    try:\n        conn = sqlite3.connect(db_path)\n        return pd.read_sql_query(query, conn)\n    finally:\n        conn.close()\n\n# Example usage\ndb = Path('data.db')\ndf = await query_database(db, '''\n    SELECT date, COUNT(*) as count\n    FROM events\n    GROUP BY date\n''')\nCommon Operations:\nDatabase Management\n-- Backup database\n.backup 'backup.db'\n\n-- Import CSV\n.mode csv\n.import data.csv table_name\n\n-- Export results\n.headers on\n.mode csv\n.output results.csv\nSELECT * FROM table;\nPerformance Optimization\n-- Create index\nCREATE INDEX idx_user_email ON users(email);\n\n-- Analyze query\nEXPLAIN QUERY PLAN\nSELECT * FROM users WHERE email LIKE '%@example.com';\n\n-- Show indexes\nSELECT * FROM sqlite_master WHERE type='index';\nData Analysis\n-- Time series aggregation\nSELECT\n    date(timestamp),\n    COUNT(*) as events,\n    AVG(duration) as avg_duration\nFROM events\nGROUP BY date(timestamp);\n\n-- Window functions\nSELECT *,\n    AVG(amount) OVER (\n        PARTITION BY user_id\n        ORDER BY date\n        ROWS BETWEEN 3 PRECEDING AND CURRENT ROW\n    ) as moving_avg\nFROM transactions;\nTools to work with SQLite:\nSQLiteStudio: Lightweight GUI\nDBeaver: Full-featured GUI\nsqlite-utils: CLI tool\nDatasette: Web interface"
  },
  {
    "section": "../git",
    "text": "Version Control: Git, GitHub\nGit is the de facto standard for version control of software (and sometimes, data as well). It’s a system that keeps track of changes you make to files and folders. It allows you to revert to a previous state, compare changes, etc. It’s a central tool in any developer’s workflow.\nGitHub is the most popular hosting service for Git repositories. It’s a website that shows your code, allows you to collaborate with others, and provides many useful tools for developers.\nWatch these introductory videos to learn the basics of Git and GitHub (98 min):\nEssential Git Commands:\n# Repository Setup\ngit init                   # Create new repo\ngit clone url              # Clone existing repo\ngit remote add origin url  # Connect to remote\n\n# Basic Workflow\ngit status                 # Check status\ngit add .                  # Stage all changes\ngit commit -m \"message\"    # Commit changes\ngit push origin main       # Push to remote\n\n# Branching\ngit branch                 # List branches\ngit checkout -b feature    # Create/switch branch\ngit merge feature          # Merge branch\ngit rebase main            # Rebase on main\n\n# History\ngit log --oneline          # View history\ngit diff commit1 commit2   # Compare commits\ngit blame file             # Show who changed what\nBest Practices:\nCommit Messages\n# Good commit message format\ntype(scope): summary\n\nDetailed description of changes.\n\n# Examples\nfeat(api): add user authentication\nfix(db): handle null values in query\nBranching Strategy\nmain: Production code\ndevelop: Integration branch\nfeature/*: New features\nhotfix/*: Emergency fixes\nCode Review\nKeep PRs small (<400 lines)\nUse draft PRs for WIP\nReview your own code first\nRespond to all comments\nEssential Tools\nGitHub Desktop: GUI client\nGitLens: VS Code extension\ngh: GitHub CLI\npre-commit: Git hooks"
  },
  {
    "section": "../deployment-tools",
    "text": "Deployment Tools\nAny application you build is likely to be deployed somewhere. This section covers the most popular tools involved in deploying an application."
  },
  {
    "section": "../github-actions",
    "text": "CI/CD: GitHub Actions\nGitHub Actions is a powerful automation platform built into GitHub. It helps automate your development workflow - running tests, deploying applications, updating datasets, retraining models, etc.\nUnderstand the basics of YAML configuration files\nExplore the pre-built actions from the marketplace\nHow to handle secrets securely\nTriggering a workflow\nStaying within the free tier limits\nCaching dependencies to speed up workflows\nHere is a sample .github/workflows/iss-location.yml that runs daily, appends the International Space Station location data into iss-location.json, and commits it to the repository.\nname: Log ISS Location Data Daily\n\non:\n  schedule:\n    # Runs at 12:00 UTC (noon) every day\n    - cron: \"0 12 * * *\"\n  workflow_dispatch: # Allows manual triggering\n\njobs:\n  collect-iss-data:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v5\n\n      - name: Fetch ISS location data\n        run: | # python\n          uv run --with requests python << 'EOF'\n          import requests\n\n          data = requests.get('http://api.open-notify.org/iss-now.json').text\n          with open('iss-location.jsonl', 'a') as f:\n              f.write(data + '\\n')\n          'EOF'\n\n      - name: Commit and push changes\n        run: | # shell\n          git config --local user.email \"github-actions[bot]@users.noreply.github.com\"\n          git config --local user.name \"github-actions[bot]\"\n          git add iss-location.jsonl\n          git commit -m \"Update ISS position data [skip ci]\" || exit 0\n          git push\nTools:\nGitHub CLI: Manage workflows from terminal\nSuper-Linter: Validate code style\nRelease Drafter: Automate releases\nact: Run actions locally\nHow to handle secrets in GitHub Actions"
  },
  {
    "section": "../docker",
    "text": "Containers: Docker, Podman\nDocker and Podman are containerization tools that package your application and its dependencies into a standardized unit for software development and deployment.\nDocker is the industry standard. Podman is compatible with Docker and has better security (and a slightly more open license). In this course, we recommend Podman but Docker works in the same way.\nInitialize the container engine:\npodman machine init\npodman machine start\nCopy to clipboard\nCommon Operations. (You can use docker instead of podman in the same way.)\n# Pull an image\npodman pull python:3.11-slim\n\n# Run a container\npodman run -it python:3.11-slim\n\n# List containers\npodman ps -a\n\n# Stop container\npodman stop container_id\n\n# Scan image for vulnerabilities\npodman scan myapp:latest\n\n# Remove container\npodman rm container_id\n\n# Remove all stopped containers\npodman container prune\nYou can create a Dockerfile to build a container image. Here’s a sample Dockerfile that converts a Python script into a container image.\nFROM python:3.11-slim\n# Set working directory\nWORKDIR /app\n# Typically, you would use `COPY . .` to copy files from the host machine,\n# but here we're just using a simple script.\nRUN echo 'print(\"Hello, world!\")' > app.py\n# Run the script\nCMD [\"python\", \"app.py\"]\nTo build, run, and deploy the container, run these commands:\n# Create an account on https://hub.docker.com/ and then login\npodman login docker.io\n\n# Build and run the container\npodman build -t py-hello .\npodman run -it py-hello\n\n# Push the container to Docker Hub. Replace $DOCKER_HUB_USERNAME with your Docker Hub username.\npodman push py-hello:latest docker.io/$DOCKER_HUB_USERNAME/py-hello\n\n# Push adding a specific tag, e.g. dev\nTAG=dev podman push py-hello docker.io/$DOCKER_HUB_USERNAME/py-hello:$TAG\nTools:\nDive: Explore image layers\nSkopeo: Work with container images\nTrivy: Security scanner\nOptional: For Windows, see WSL 2 with Docker getting started"
  },
  {
    "section": "../large-language-models",
    "text": "Large Language Models\nThis module covers the practical usage of large language models (LLMs).\nLLMs incur a cost. For the May 2025 batch, use aipipe.org as a proxy. Emails with @ds.study.iitm.ac.in get a $1 per calendar month allowance. (Don’t exceed that.)\nRead the AI Pipe documentation to learn how to use it. But in short:\nReplace OPENAI_BASE_URL, i.e. https://api.openai.com/v1 with https://aipipe.org/openrouter/v1... or https://aipipe.org/openai/v1...\nReplace OPENAI_API_KEY with the AIPIPE_TOKEN\nReplace model names, e.g. gpt-4.1-nano, with openai/gpt-4.1-nano\nFor example, let’s use Gemini 2.0 Flash Lite via OpenRouter for chat completions and Text Embedding 3 Small via OpenAI for embeddings:\ncurl https://aipipe.org/openrouter/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $AIPIPE_TOKEN\" \\\n  -d '{\n    \"model\": \"google/gemini-2.0-flash-lite-001\",\n    \"messages\": [{ \"role\": \"user\", \"content\": \"What is 2 + 2?\"} }]\n  }'\n\ncurl https://aipipe.org/openai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $AIPIPE_TOKEN\" \\\n  -d '{ \"model\": \"text-embedding-3-small\", \"input\": \"What is 2 + 2?\" }'\nOr using llm:\nllm keys set openai --value $AIPIPE_TOKEN\n\nexport OPENAI_BASE_URL=https://aipipe.org/openrouter/v1\nllm 'What is 2 + 2?' -m openrouter/google/gemini-2.0-flash-lite-001\n\nexport OPENAI_BASE_URL=https://aipipe.org/openai/v1\nllm embed -c 'What is 2 + 2' -m 3-small\nFor a 50% discount (but slower speed), use Flex processing by adding service_tier: \"flex\" to your JSON request.\nAI Proxy - Jan 2025\nFor the Jan 2025 batch, we had created API keys for everyone with an iitm.ac.in email to use gpt-4o-mini and text-embedding-3-small. Your usage is limited to $1 per calendar month for this course. Don’t exceed that.\nUse AI Proxy instead of OpenAI. Specifically:\nReplace your API to https://api.openai.com/... with https://aiproxy.sanand.workers.dev/openai/...\nReplace the OPENAI_API_KEY with the AIPROXY_TOKEN that someone will give you."
  },
  {
    "section": "../retrieval-augmented-generation",
    "text": "Retrieval Augmented Generation\nThe video is not available yet. Please review the notebook, which is self-explanatory. #TODO\nYou will learn to implement Retrieval Augmented Generation (RAG) to enhance language models’ responses by incorporating relevant context, covering:\nLLM Context Limitations: Understanding the constraints of context windows in large language models.\nRetrieval Augmented Generation: The technique of retrieving and using relevant documents to enhance model responses.\nEmbeddings: How to convert text into numerical representations that are used for similarity calculations.\nSimilarity Search: Finding the most relevant documents by calculating cosine similarity between embeddings.\nOpenAI API Integration: Using the OpenAI API to generate responses based on the most relevant documents.\nTourist Recommendation Bot: Building a bot that recommends tourist attractions based on user interests using embeddings.\nNext Steps for Implementation: Insights into scaling the solution with a vector database, re-rankers, and improved prompts for better accuracy and efficiency.\nHere are the links used in the video:\nJupyter Notebook\ngte-large-en-v1.5 embedding model\nAwesome vector database"
  },
  {
    "section": "../data-sourcing",
    "text": "Data Sourcing\nBefore you do any kind of data science, you obviously have to get the data to be able to analyze it, visualize it, narrate it, and deploy it. And what we are going to cover in this module is how you get the data.\nThere are three ways you can get the data.\nThe first is you can download the data. Either somebody gives you the data and says download it from here, or you are asked to download it from the internet because it’s a public data source. But that’s the first way—you download the data.\nThe second way is you can query it from somewhere. It may be on a database. It may be available through an API. It may be available through a library. But these are ways in which you can selectively query parts of the data and stitch it together.\nThe third way is you have to scrape it. It’s not directly available in a convenient form that you can query or download. But it is, in fact, on a web page. It’s available on a PDF file. It’s available in a Word document. It’s available on an Excel file. It’s kind of structured, but you will have to figure out that structure and extract it from there.\nIn this module, we will be looking at the tools that will help you either download from a data source or query from an API or from a database or from a library. And finally, how you can scrape from different sources.\nHere are links used in the video:\nThe Movies Dataset\nIMDb Datasets\nDownload the IMDb Datasets\nExplore the Internet Movie Database\nWhat does the world search for?\nHowStat - Cricket statistics\nCricket Strike Rates"
  },
  {
    "section": "../scraping-pdfs-with-tabula",
    "text": "Scraping PDFs with Tabula\nYou’ll learn how to scrape tables from PDFs using the tabula Python library, covering:\nImport Libraries: Use Beautiful Soup for URL parsing and Tabula for extracting tables from PDFs.\nSpecify Save Location: Mount Google Drive to save scraped PDFs.\nIdentify PDF URLs: Parse the given URL to identify and select all PDF links.\nDownload PDFs: Loop through identified links, saving each PDF to the specified location.\nExtract Tables: Use Tabula to read tabular content from the downloaded PDFs.\nControl Extraction Area: Specify page and area coordinates to accurately extract tables, avoiding extraneous text.\nSave Extracted Data: Convert the extracted table data into structured formats like CSV for further analysis.\nHere are links and references:\nPDF Scraping - Notebook\nLearn about the tabula package\nLearn about the pandas package. Video"
  },
  {
    "section": "../data-preparation",
    "text": "Data Preparation\nData preparation is crucial because raw data is rarely perfect.\nIt often contains errors, inconsistencies, or missing values. For example, marks data may have ‘NA’ or ‘absent’ for non-attendees, which you need to handle.\nThis section teaches you how to clean up data, convert it to different formats, aggregate it if required, and get a feel for the data before you analyze.\nHere are links used in the video:\nPresentation used in the video\nScraping assembly elections - Notebook\nAssembly election results (CSV)\npdftotext software\nOpenRefine software\nThe most persistent party\nTN assembly election cartogram"
  },
  {
    "section": "../data-analysis",
    "text": "Data analysis\nData Analysis: Introduction Podcast by NotebookLM\nOnce you’ve prepared the data, your next task is to analyze it to get insights that are not immediately obvious.\nIn this module, you’ll learn:\nStatistical analysis: Calculate correlations, regressions, forecasts, and outliers using spreadsheets\nData summarization: Aggregate and pivot data using Python and databases.\nGeo-data Collection & Processing: Gather and process geospatial data using tools like Python (GeoPandas) and QGIS.\nGeo-visualization: Create and visualize geospatial data on maps using Excel, QGIS, and Python libraries such as Folium.\nNetwork & Proximity Analysis: Analyze geospatial relationships and perform network analysis to understand data distribution and clustering.\nStorytelling & Decision Making: Develop narratives and make informed decisions based on geospatial data insights."
  },
  {
    "section": "../project-2",
    "text": "Project 2 - TDS Solver\nThis project is due on 31 Mar 2025 EoD IST. Results will be announced by 15 Apr 2025.\nFor questions, use this Discourse thread.\nBackground\nYou are a clever student who has joined IIT Madras’ Online Degree in Data Science. You have just enrolled in the Tools in Data Science course.\nTo make your life easier, you have decided to build an LLM-based application that can automatically answer any of the graded assignment questions.\nSpecifically, you are building and deploying an API that accepts any question from one of these 5 graded assignments:\nGraded Assignment 1\nGraded Assignment 2\nGraded Assignment 3\nGraded Assignment 4\nGraded Assignment 5\n… and responds with the answer to enter in the assignment.\nCreate an API\nYour application exposes an API endpoint. Let’s assume that it is at https://your-app.vercel.app/api/.\nThe endpoint must accept a POST request, e.g. POST https://your-app.vercel.app/api/ with the question as well as optional file attachments as multipart/form-data.\nFor example, here’s how anyone can make a request:\ncurl -X POST \"https://your-app.vercel.app/api/\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -F \"question=Download and unzip file abcd.zip which has a single extract.csv file inside. What is the value in the \"answer\" column of the CSV file?\" \\\n  -F \"file=@abcd.zip\"\nThe response must be a JSON object with a single text (string) field: answer that can be directly entered in the assignment. For example:\n{\n  \"answer\": \"1234567890\"\n}\nDeploy your application\nDeploy your application to a public URL that can be accessed by anyone. You may use any platform, including Vercel.\n(If you use ngrok, ensure that it is running continuously until you get your results.)\nShare your code\nCreate a new public GitHub repository\nAdd an MIT LICENSE file\nCommit and push your code\nSubmit your URL\nSubmit your GitHub repository URL and your API endpoint URL in this Google Form: https://forms.gle/6ZLCGEEHUHVK71Yu5.\nEvaluation\nPre-requisites: Your repository MUST meet the following criteria to be eligible for evaluation\nYour GitHub repository exists and is publicly accessible\nYour GitHub repository has a LICENSE file with the MIT license\nWe will send 5 questions randomly chosen from the graded assignments above. Correct answers will be awarded 4 marks each.\nYour score will be the sum of the marks above. No normalization. What you get is what you get."
  },
  {
    "section": "../data-visualization",
    "text": "Data visualization"
  },
  {
    "section": "../live-session-2025-01-28",
    "text": "Live Session: 28 Jan 2025\nQ1: Can I ask a question about the graded assignment for week one?\nA1: I’ll try to answer it at the end of the session if there’s time. We’ll also have extra sessions this week, and we’re hoping to release video solutions showing how to solve some of the assignments. But for now, let’s focus on today’s session.\nQ2: I posted a question on the Discourse forum about the assignment. Should I ask it again now?\nA2: No need to ask it again. I’ll look at the forum later.\nQ3: DeepSeek is in the news. Does that affect the course?\nA3: Not really. The core concept remains the same: using LLMs as agents. The workflow is similar whether you use OpenAI, Anthropic, Copilot, or other AI tools.\nQ4: What is the concept behind Project One?\nA4: Project One involves creating an automated continuous integration pipeline. Data comes into a server, and various tasks are performed, many of which are repetitive. The goal is to automate some of this process to reduce errors and produce structured output. Your job is to build an API that handles these tasks using an LLM.\nQ5: What kind of tasks can I expect?\nA5: There are roughly 10 tasks in Phase A, which should be straightforward. In Phase B, the tasks are less well-defined, testing your system’s ability to handle unexpected requests. The API should have two endpoints: “run” and “read”. The “run” endpoint receives a task description (in plain English), processes it, and creates output files. The “read” endpoint retrieves the contents of a specified file.\nQ6: In GA1, the videos and questions aren’t showing after the submission deadline. Can they be shown?\nA6: I’ll inform Anand. The videos should still be available on the Tools in Data Science portal. They’re from this section.\nQ7: Why are the videos and questions integrated into the GA itself?\nA7: Based on past data, students often go straight to the assignments without reviewing the content. Integrating the content into the GA is a better way to ensure they see it.\nQ8: I’m struggling with the project. The concepts are confusing. Could you present a model project and explain things step-by-step?\nA8: Yes, we’ll do that in future sessions. We’ll work through a working prototype of the project to illustrate each step. The project only uses material from the first three weeks of the course. If you’re thorough with that material, you should be able to complete the project. It seems there’s a knowledge gap, but I have full faith you can handle it. We’ll address that gap in future sessions, starting with the basics.\nQ9: I’m having trouble with Git and GitHub. Is there support available?\nA9: Yes, we’ll provide as much support as possible. We’ll also go through a working prototype in future sessions. If you’re struggling with Git, you won’t be able to do the project, as it’s a prerequisite. We’ll create a repository during a future session so you can see how it’s done.\nQ10: What’s required for grading?\nA10: Your repository must meet these criteria:\nIt must exist and be publicly accessible.\nIt must have a license file (MIT license).\nIt must have a valid Dockerfile.\nThe Docker image must be publicly accessible and run via the specified command.\nThe Docker image must use the same Dockerfile as your GitHub repository.\nIt doesn’t have to be publicly accessible while you’re developing it. Just make sure it’s public when you submit. These are absolute prerequisites; if any fail, your project won’t be evaluated.\nQ11: What is the grading scheme?\nA11: Phase A is worth 10 marks, and Phase B is worth 10 marks. You get one mark for each task completed in Phase A and one for each task in Phase B. There are also bonus marks for additional tasks and code diversity. Last term, there were 12 bonus marks, and the highest score was 7 or 8 out of 12.\nQ12: We give bonuses for code diversity. Does that mean we penalize copying?\nA12: No, we don’t penalize copying. We want you to learn. We encourage you to learn from each other, but we’ll reward unique code.\nQ13: How do I use the OpenAI LLM?\nA13: You’ll use it through a proxy. You’ll get a $1 limit per month, renewed at the beginning of each month. Avoid exceeding this limit by carefully constructing your queries. We’ll show you how to use environment variables to avoid putting your token directly in your code.\nQ14: How do I get access to Perplexity AI?\nA14: All IITM students (including BS students) have free access to the Pro version of Perplexity AI for one year. Use your IITM email address to register.\nQ15: In the LLM section, the prompt engineering section mentions using the API a certain number of times. I haven’t used it even once. The key was generated, but I didn’t use it. What should I do?\nA15: We’ll cover this in a later session.\nQ16: The textbox in question 3 doesn’t appear. I think there’s a CSS issue. I generated the JSON, but it’s not being accepted. Should I refresh?\nA16: We’ll cover this in a later session.\nQ17: There was mention of a debrief session on the project. I couldn’t find the video. Is there one?\nA17: Yes, at the beginning of this session, we discussed the requirements for Project One. We’ll upload this video in a few hours or early tomorrow.\nQ18: Will there be support for those not well-versed in Git and GitHub?\nA18: Yes, we’ll provide support. We’ll also go through a working prototype in future sessions.\nQ19: Is the week two content completely covered in the sessions?\nA19: Yes, but the way we covered it is a bit different. We went through the important parts. Referencing past sessions will give you a concise idea of the important parts.\nQ20: I have a question about GA1, questions 10 and 11. I posted them on Discourse, but haven’t received a reply. In question 10, the hash button is showing an error. In question 11, I don’t understand the statement about the class in the hidden element below. What does this line mean?\nA20: Regarding question 10, you have to use inverted double quotes for the key and value in the JSON file. Regarding question 11, you need to right-click and inspect the element to find the class name. The data value should be 35.\nQ21: So, for this one, I don’t get anything. Did I do it correctly? Should I submit anything else?\nA21: You just say “yes” here, and then submit to the URL. It will only pick up the API itself. Remove the question mark.\nQ22: Why isn’t it reflecting?\nA22: It may be because you haven’t pushed to GitHub yet.\nQ23: I have a question about prompt engineering. It mentions using the API a certain number of times. I haven’t used it, but it generated a key. Can I proceed?\nA23: No, you don’t need to use the API multiple times. The free OpenAI API access is only through the proxy. A normal OpenAI account is a subscription service and doesn’t give you API access. You have to buy tokens to use the API.\nQ24: I’ve used a normal OpenAI account before. I was hoping to use those model IDs. Why can’t I?\nA24: The free OpenAI account doesn’t give you API access. You can only use their web-based service. To use the API, you have to buy tokens.\nQ25: Can you show me how to do that? Can I share my screen?\nA25: No need to share your screen. I’ll show you how to access the API via the proxy. That’s how your project will have to work.\nQ26: The ninth question in the prompt engineering section mentions paying money. Do I have to pay?\nA26: No, you don’t have to pay.\nQ27: I have a few more questions. Will we have more sessions before GA3 is due?\nA27: Yes, we’ll have at least three more sessions before GA3 is due.\nQ28: I thought GA3 was due this weekend. Is that wrong?\nA28: Yes, it’s due this weekend, but we’ll have three more sessions before then.\nQ29: I have another question. Is it okay to ask now?\nA29: Sure.\nQ30: I submitted my answers, but I don’t know if I did it correctly. There’s nothing to submit here. I just say “yes”?\nA30: You just say “yes” and submit to the URL. It will pick up the API itself.\nQ31: Why isn’t it reflecting?\nA31: It may be because you haven’t pushed to GitHub yet.\nQ32: I have a question about GA1, questions 10 and 11. I posted them on Discourse, but haven’t received a reply. In question 10, the hash button is showing an error. In question 11, I don’t understand the statement about the class in the hidden element below. What does this line mean?\nA32: For question 10, you need to use inverted double quotes for the key and value in the JSON file. For question 11, you need to right-click and inspect the element to find the class name. The data value should be 35.\nQ33: What are some other core concepts that are important to understand?\nA33: Before I talk about those, let’s start with a simple example of how to make an API call to an LLM. API calls are powerful because you can use them in your programs.\nQ34: How do I get my API key?\nA34: You can find the link to get your proxy token on the GitHub page. I’ll demonstrate using this proxy, but you can use something similar for other LLMs.\nQ35: What is a proxy?\nA35: OpenAI provides the service, but you don’t interact with them directly. Anand has purchased tokens from OpenAI and provides access via a proxy. The proxy acts as a middleman between you and OpenAI.\nQ36: How many tokens does a prompt take?\nA36: The prompt we used took 32 tokens. The response (“negative”) took two tokens. The total was 34 tokens. This cost us 1/10,000 of a dollar. Keep track of your token usage.\nQ37: How can I keep track of token usage?\nA37: Keep track of the prompt and the cost in a file. This will help you be efficient.\nQ38: How are API calls made?\nA38: You’ll need a URL, headers (including authorization), and a JSON payload. The payload includes the model, messages, and response format. The response format should be strictly defined to avoid unexpected output.\nQ39: Can you share the notebook file?\nA39: No, we don’t typically share the notebook file itself, as it prevents you from fully grasping the concepts. However, a sample file is available.\nQ40: What about function calling?\nA40: We’ll cover function calling in a later session. It’s a key part of the project. Function calling allows your LLM to decide which function to call based on the prompt. We’ll use last term’s Project Two to demonstrate.\nQ41: What about embeddings?\nA41: Embeddings are another important topic. They reduce the cost of using tokens by an order of magnitude. You can download embeddings from Hugging Face. We’ll demonstrate this in a future session.\nQ42: What about text extraction?\nA42: We’ll cover this in a future session.\nQ43: What about Base64 encoding?\nA43: We’ll cover this in a future session. It’s how you send images to the API.\nQ44: I’ve seen students use the LLM to get answers to Quiz One. Can I do that?\nA44: Yes, you can upload a screenshot of the question and ask the LLM for the answer and explanation. You would send it as a Base64 encoded URI. We’ll cover this in a future session.\nQ45: Can we simply go through the topics mentioned (prompt engineering, TDS, TA instructions, LLM sentiment analysis) during self-study?\nA45: Yes, but this content is simplified. Reading through documentation is difficult, so we’ve put this together for you. We’ll also demonstrate and do working examples.\nQ46: Can we change the time of the sessions? My other classes are clashing. I’ve already told the TA, but haven’t received a response.\nA46: The Wednesday session cannot be moved. The Thursday session might be possible, but the sessions are generally kept in the evening because many students work during the day. We record all sessions, so you can view them later. We can try to schedule an earlier doubt-clearing session, perhaps on Friday, but I’ll have to discuss it with the team. What time on Friday works for you?"
  }
]